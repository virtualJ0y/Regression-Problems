# -*- coding: utf-8 -*-
"""assignment1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zFUJ5xtFGjNyzncMdx_nB48GP-zGvXGB
"""

#libraries
import numpy as np                 #helps with numerical handling
import math       
import matplotlib.pyplot as plt    #helps with plots
import scipy.optimize as opt       #curve fitting tools

#generation and sorting of the input values
numberOfGeneratedValues = 150
inputValues = np.random.uniform(-4.,4., numberOfGeneratedValues)
inputValues.sort()

def myCustFunc(x,l1,l2):
    y = np.array([])
    for i in range(len(x)):
      y = np.append(y,(l1 * 1/math.exp(x[i]) + l2 * math.sin(x[i])))
    return y

#generation of the output values
outputValues = myCustFunc(inputValues,0.2,0.4)

#plotting the data
plt.figure(figsize=(12,4))
plt.plot(inputValues, outputValues, 'g-', label='MyCustData')
plt.xlabel('Randomized input values')
plt.ylabel('Resulsts of myCustFunc')
plt.legend(loc='best')
plt.show()

#adding noise to the input data that takes values from 0 to 1
noise = np.random.uniform(size=150)
noisyInputValues = inputValues + noise

#fitting the parameters for myCustFunc
bestValsLinModel, CoVarLinModel = opt.curve_fit(myCustFunc,inputValues,noisyInputValues)
print('linear model estimated parameters (on noisy data) are:',\
      '{:.2f}'.format(bestValsLinModel[0]),\
      '{:.2f}'.format(bestValsLinModel[1]))
linnearModelPredictedOutcomes = myCustFunc(inputValues, bestValsLinModel[0], bestValsLinModel[1])

"""# New Section"""

#defining function that returns a 4th degree polynomial
def poly4thdegree(x, a, b, c, d, e):
  return a*(x**4) + b*(x**3) + c*(x**2) + d*x + e

#fitting the parameters for poly4thdegree
bestValsLinModel4, CoVarLinModel4 = opt.curve_fit(poly4thdegree,inputValues,noisyInputValues)
print('4th degree polynomial model estimated parameters (on noisy data) are:',\
      '{:.2f}'.format(bestValsLinModel4[0]),\
      '{:.2f}'.format(bestValsLinModel4[1]),\
      '{:.2f}'.format(bestValsLinModel4[2]),\
      '{:.2f}'.format(bestValsLinModel4[3]),\
      '{:.2f}'.format(bestValsLinModel4[4]))
poly4ModelPredictedOutcomes = poly4thdegree(inputValues, bestValsLinModel4[0], bestValsLinModel4[1], bestValsLinModel4[2], bestValsLinModel4[3], bestValsLinModel4[4])

#ploting the noisy input values and the results of myCustFunc with the best estimated parameters
plt.plot(noisyInputValues,linnearModelPredictedOutcomes, '.', label='myCustFunc')
plt.xlabel ('noisy input')
plt.ylabel ('output of myCustFunc')
plt.title ('results of myCustFunc with the best estimated parameters ')
plt.legend(loc='best')
plt.show()

#ploting the noisy input values and the results of poly4thdegree with the best estimated parameters
plt.plot(noisyInputValues,poly4ModelPredictedOutcomes, '.', label='poly4thdegree')
plt.xlabel ('noisy input')
plt.ylabel ('output of poly4thdegree')
plt.title ('results of poly4thdegree with the best estimated parameters ')
plt.legend(loc='best')
plt.show()

#calculation of mean absolute errors and root squared errors
from sklearn.metrics import mean_absolute_error, mean_squared_error

mae_a = mean_absolute_error(inputValues, noisyInputValues)
rmse_a = np.sqrt(mean_squared_error(inputValues, noisyInputValues))

print('The statistical errors for original values - noisy values are:',\
      'MAE: {:.2f}'.format(mae_a),\
      'RMSE: {:.2f}'.format(rmse_a))

mae_b = mean_absolute_error(noisyInputValues, linnearModelPredictedOutcomes)
rmse_b = np.sqrt(mean_squared_error(noisyInputValues, linnearModelPredictedOutcomes))

print('The statistical errors for noisy values - myCustFunc are:',\
      'MAE: {:.2f}'.format(mae_b),\
      'RMSE: {:.2f}'.format(rmse_b))

mae_c = mean_absolute_error(noisyInputValues, poly4ModelPredictedOutcomes)
rmse_c = np.sqrt(mean_squared_error(noisyInputValues, poly4ModelPredictedOutcomes))

print('The statistical errors for noisy values - poly4thDegree are:',\
      'MAE: {:.2f}'.format(mae_c),\
      'RMSE: {:.2f}'.format(rmse_c))

#Estimation without knowledge of the function (ML approach)

#separation of the data to train, validation and test sets
trainDatPerc = 0.6
valDatPerc = trainDatPerc/10
testDatPec = 1 - trainDatPerc - valDatPerc

# create shufled indexes for the data 
indices = np.random.permutation(len(inputValues))
trainInd = indices[0:int(trainDatPerc*len(indices))]
valInd = indices[int(trainDatPerc*len(indices)+1):\
                 int((trainDatPerc+valDatPerc)*len(indices))]
testInd = indices[int((trainDatPerc+valDatPerc)*len(indices))+1:]

#reshaping the arrays in order for them to be in the proper form to be used as input
noisyInputValues = noisyInputValues.reshape(-1,1)
inputValues = inputValues.reshape(-1,1)

#kNN regressor
from sklearn.neighbors import KNeighborsRegressor

# Initialize a model with 5 neighbors
knnReg = KNeighborsRegressor(n_neighbors=5)

# Fit the model
knnReg.fit(inputValues[trainInd],noisyInputValues[trainInd])

# Predict the values
predictedOutputsknn = knnReg.predict(inputValues[testInd])

mae = mean_absolute_error(noisyInputValues[testInd],predictedOutputsknn)
print ("The MAE of the kNN regressor is: {:.2f}".format(mae))
mse = mean_squared_error(noisyInputValues[testInd],predictedOutputsknn)
print ("The MSE of the kNN regressor is: {:.2f}".format(mse))

#SVR
from sklearn.svm import SVR

# Initialize a model with rbf kernel
svmReg =SVR(kernel='rbf')

# Fit the model
svmReg.fit(inputValues[trainInd],noisyInputValues[trainInd].ravel())

# Predict the values
predictedOutputsSvr = svmReg.predict(inputValues[testInd])

mae = mean_absolute_error(noisyInputValues[testInd],predictedOutputsSvr)
print ("The MAE of the SVR regressor is: {:.2f}".format(mae))
mse = mean_squared_error(noisyInputValues[testInd],predictedOutputsSvr)
print ("The MSE of the SVR regressor is: {:.2f}".format(mse))

#CART — Classification and Regression Trees
from sklearn.tree import DecisionTreeRegressor

DTReg = DecisionTreeRegressor()
DTReg.fit(inputValues[trainInd],noisyInputValues[trainInd])

# Predict the values
predictedOutputsCart = DTReg.predict(inputValues[testInd])

mae = mean_absolute_error(noisyInputValues[testInd],predictedOutputsCart)
print ("The MAE of the CART regressor is: {:.2f}".format(mae))
mse = mean_squared_error(noisyInputValues[testInd],predictedOutputsCart)
print ("The MSE of the CART regressor is: {:.2f}".format(mse))

#comparison of the actual results with the results of the regressors
plt.figure(figsize=(12, 4)) 
plt.plot(noisyInputValues[testInd],'.', label = 'actual')
plt.plot(predictedOutputsknn,'rx', label = 'knn')
plt.plot(predictedOutputsSvr,'g+', label = 'svr')
plt.plot(predictedOutputsCart,'b*', label = "cart" )
plt.legend(loc='best')
plt.show()

#same as above but after normalizing the inputs

#normalize the input values using a build in scaler
from sklearn.preprocessing import MinMaxScaler
scaler_inputs = MinMaxScaler()
inputValues = scaler_inputs.fit_transform(inputValues)
noisyInputValues = scaler_inputs.fit_transform(noisyInputValues)

#separation of the data to train, validation and test sets
trainDatPerc = 0.6
valDatPerc = trainDatPerc/10
testDatPec = 1 - trainDatPerc - valDatPerc

# create shufled indexes for the data 
indices = np.random.permutation(len(inputValues))
trainInd = indices[0:int(trainDatPerc*len(indices))]
valInd = indices[int(trainDatPerc*len(indices)+1):\
                 int((trainDatPerc+valDatPerc)*len(indices))]
testInd = indices[int((trainDatPerc+valDatPerc)*len(indices))+1:]

#reshape the arrays
noisyInputValues = noisyInputValues.reshape(-1,1)
inputValues = inputValues.reshape(-1,1)

#kNN regressor
from sklearn.neighbors import KNeighborsRegressor

# Initialize a model with 5 neighbors
knnReg = KNeighborsRegressor(n_neighbors=5)

# Fit the model
knnReg.fit(inputValues[trainInd],noisyInputValues[trainInd])

# Predict the values
predictedOutputsknn = knnReg.predict(inputValues[testInd])

mae = mean_absolute_error(noisyInputValues[testInd],predictedOutputsknn)
print ("The MAE of the kNN regressor is: {:.2f}".format(mae))
mse = mean_squared_error(noisyInputValues[testInd],predictedOutputsknn)
print ("The MSE of the kNN regressor is: {:.2f}".format(mse))

#SVR
from sklearn.svm import SVR

# Initialize a model with rbf kernel
svmReg =SVR(kernel='rbf')

# Fit the model
svmReg.fit(inputValues[trainInd],noisyInputValues[trainInd].ravel())

# Predict the values
predictedOutputsSvr = svmReg.predict(inputValues[testInd])

mae = mean_absolute_error(noisyInputValues[testInd],predictedOutputsSvr)
print ("The MAE of the SVR regressor is: {:.2f}".format(mae))
mse = mean_squared_error(noisyInputValues[testInd],predictedOutputsSvr)
print ("The MSE of the SVR regressor is: {:.2f}".format(mse))

#CART — Classification and Regression Trees
from sklearn.tree import DecisionTreeRegressor

DTReg = DecisionTreeRegressor()
DTReg.fit(inputValues[trainInd],noisyInputValues[trainInd])

# Predict the values
predictedOutputsCart = DTReg.predict(inputValues[testInd])

mae = mean_absolute_error(noisyInputValues[testInd],predictedOutputsCart)
print ("The MAE of the CART regressor is: {:.2f}".format(mae))
mse = mean_squared_error(noisyInputValues[testInd],predictedOutputsCart)
print ("The MSE of the CART regressor is: {:.2f}".format(mse))

#comparison of the actual results with the results of the regressors, after normalization
plt.figure(figsize=(12, 4)) 
plt.plot(noisyInputValues[testInd],'.', label = 'actual')
plt.plot(predictedOutputsknn,'rx', label = 'knn')
plt.plot(predictedOutputsSvr,'g+', label = 'svr')
plt.plot(predictedOutputsCart,'b*', label = "cart" )
plt.legend(loc='best')
plt.show()

